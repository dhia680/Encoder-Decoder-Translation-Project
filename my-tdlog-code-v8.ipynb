{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{},"version_major":2,"version_minor":0}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":128696,"sourceType":"datasetVersion","datasetId":64315},{"sourceId":4575591,"sourceType":"datasetVersion","datasetId":2669072},{"sourceId":7333476,"sourceType":"datasetVersion","datasetId":4257237}],"dockerImageVersionId":30628,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **I. Imports**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport random\nimport string\nimport re\nimport time \nimport tensorflow as tf\nimport tensorflow.data as tf_data   #not used now\nimport tensorflow.strings as tf_strings   #not used now\n\nimport tensorflow.keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import TextVectorization\n\nimport numpy as np\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\nfrom tensorflow.keras.optimizers import RMSprop\nfrom gensim.models import Word2Vec\n\n# import keras.ops as ops\nimport joblib","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:12.213417Z","iopub.execute_input":"2024-01-14T21:19:12.214289Z","iopub.status.idle":"2024-01-14T21:19:12.226817Z","shell.execute_reply.started":"2024-01-14T21:19:12.214258Z","shell.execute_reply":"2024-01-14T21:19:12.226124Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"#### **Checking GPU availabiliy**","metadata":{}},{"cell_type":"code","source":"# from keras import backend as K\n# K.tensorflow_backend._get_available_gpus()\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:12.230874Z","iopub.execute_input":"2024-01-14T21:19:12.231510Z","iopub.status.idle":"2024-01-14T21:19:12.243178Z","shell.execute_reply.started":"2024-01-14T21:19:12.231485Z","shell.execute_reply":"2024-01-14T21:19:12.242218Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 10520094975519231041\nxla_global_id: -1\n, name: \"/device:GPU:0\"\ndevice_type: \"GPU\"\nmemory_limit: 14626652160\nlocality {\n  bus_id: 1\n  links {\n    link {\n      device_id: 1\n      type: \"StreamExecutor\"\n      strength: 1\n    }\n  }\n}\nincarnation: 12785755483320340023\nphysical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\nxla_global_id: 416903419\n, name: \"/device:GPU:1\"\ndevice_type: \"GPU\"\nmemory_limit: 14626652160\nlocality {\n  bus_id: 1\n  links {\n    link {\n      type: \"StreamExecutor\"\n      strength: 1\n    }\n  }\n}\nincarnation: 16816183578209216759\nphysical_device_desc: \"device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\"\nxla_global_id: 2144165316\n]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **II. Data Extraction & Visualization**","metadata":{}},{"cell_type":"code","source":"file = '/kaggle/input/tabdelimited-englisharabic-sentence-pairs/fra.txt'","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:12.245425Z","iopub.execute_input":"2024-01-14T21:19:12.245763Z","iopub.status.idle":"2024-01-14T21:19:12.253385Z","shell.execute_reply.started":"2024-01-14T21:19:12.245721Z","shell.execute_reply":"2024-01-14T21:19:12.252461Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# df_raw = pd.read_csv(\"fra.txt\", delimiter='\\t', error_bad_lines=False, header=None, names=['en', 'fr'], index_col=False)\ndf_raw = pd.read_csv(file, delimiter='\\t', encoding='utf-8', header=None, names=['en', 'fr'], index_col=False)\ndf_raw.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:12.276465Z","iopub.execute_input":"2024-01-14T21:19:12.276712Z","iopub.status.idle":"2024-01-14T21:19:12.603172Z","shell.execute_reply.started":"2024-01-14T21:19:12.276691Z","shell.execute_reply":"2024-01-14T21:19:12.602285Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"     en          fr\n0   Go.        Va !\n1   Hi.     Salut !\n2  Run!     Cours !\n3  Run!    Courez !\n4  Wow!  Ça alors !","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>fr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Go.</td>\n      <td>Va !</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Hi.</td>\n      <td>Salut !</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Run!</td>\n      <td>Cours !</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Run!</td>\n      <td>Courez !</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Wow!</td>\n      <td>Ça alors !</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"Choosing a subset of the dataframe to work with (eventually choosing the whole dataframe)\"\"\"\nlang1 = 'en'\nlang2 = 'fr'  #subject to changes\n# Create a new DataFrame with every fourth row (50% of th data)\n# df_raw1 = df_raw.iloc[::2, :].reset_index(drop=True)\ndf_raw1 = df_raw\nprint(f\"Number of sentences : {df_raw1.count()[0]}\")\nprint(f\"English longest sentence: {df_raw1[lang1].str.len().max()}\")\nprint(f\"French longest sentence: {df_raw1[lang2].str.len().max()}\") \ndf_raw1.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:12.604544Z","iopub.execute_input":"2024-01-14T21:19:12.604863Z","iopub.status.idle":"2024-01-14T21:19:12.834929Z","shell.execute_reply.started":"2024-01-14T21:19:12.604837Z","shell.execute_reply":"2024-01-14T21:19:12.833941Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Number of sentences : 160538\nEnglish longest sentence: 286\nFrench longest sentence: 349\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_42/3154719994.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  print(f\"Number of sentences : {df_raw1.count()[0]}\")\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"     en          fr\n0   Go.        Va !\n1   Hi.     Salut !\n2  Run!     Cours !\n3  Run!    Courez !\n4  Wow!  Ça alors !","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>fr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Go.</td>\n      <td>Va !</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Hi.</td>\n      <td>Salut !</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Run!</td>\n      <td>Cours !</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Run!</td>\n      <td>Courez !</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Wow!</td>\n      <td>Ça alors !</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(df_raw1[\"en\"][len(df_raw1)-1],\"\\n\")\nprint(df_raw1[\"fr\"][len(df_raw1)-1])","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:12.835920Z","iopub.execute_input":"2024-01-14T21:19:12.836206Z","iopub.status.idle":"2024-01-14T21:19:12.842042Z","shell.execute_reply.started":"2024-01-14T21:19:12.836183Z","shell.execute_reply":"2024-01-14T21:19:12.841007Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"It may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort. However, if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning, we might be able to minimize errors. \n\nIl est peut-être impossible d'obtenir un Corpus complètement dénué de fautes, étant donnée la nature de ce type d'entreprise collaborative. Cependant, si nous encourageons les membres à produire des phrases dans leurs propres langues plutôt que d'expérimenter dans les langues qu'ils apprennent, nous pourrions être en mesure de réduire les erreurs.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **III. Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"#Returning a list of tuples (corresponding eng-fre sentences)\ndef Create_pairs(dataframe):\n    text_pairs = [(row['en'], \"[start] \"+row['fr']+\" [end]\") for index, row in dataframe.iterrows()] \n    return text_pairs","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:12.844436Z","iopub.execute_input":"2024-01-14T21:19:12.844743Z","iopub.status.idle":"2024-01-14T21:19:12.852324Z","shell.execute_reply.started":"2024-01-14T21:19:12.844712Z","shell.execute_reply":"2024-01-14T21:19:12.850703Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"##########useless for now#############\n# import spacy\n# # nlp = spacy.load('en',disable=['parser', 'tagger','ner'])  #deprecated on spacy v3\n# # nlp = spacy.load('en_core_web_sm')\n# nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\", \"tagger\", \"ner\"])\n# nlp.max_length = 1198623\n\ndef separate_punc(doc_text):\n    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:12.853688Z","iopub.execute_input":"2024-01-14T21:19:12.854038Z","iopub.status.idle":"2024-01-14T21:19:12.863511Z","shell.execute_reply.started":"2024-01-14T21:19:12.854009Z","shell.execute_reply":"2024-01-14T21:19:12.862478Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Create the pairs\ntext_pairs = Create_pairs(df_raw1)","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:12.864921Z","iopub.execute_input":"2024-01-14T21:19:12.865576Z","iopub.status.idle":"2024-01-14T21:19:22.521775Z","shell.execute_reply.started":"2024-01-14T21:19:12.865536Z","shell.execute_reply":"2024-01-14T21:19:22.521019Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#How sentence pairs look like\nfor i in range(3):\n    pair = random.choice(text_pairs)\n    print(pair)\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:22.522852Z","iopub.execute_input":"2024-01-14T21:19:22.523152Z","iopub.status.idle":"2024-01-14T21:19:22.528409Z","shell.execute_reply.started":"2024-01-14T21:19:22.523127Z","shell.execute_reply":"2024-01-14T21:19:22.527623Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(\"You're upset.\", '[start] Tu es contrariée. [end]')\n\n('Everybody is supposed to know the law, but few people really do.', '[start] Tout le monde est censé connaître les lois, mais très peu de gens les connaissent vraiment. [end]')\n\n('The GDP of China still pales in comparison with that of the US.', '[start] Le PIB de la Chine est encore dérisoire en comparaison de celui des États-Unis. [end]')\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# en_sentences = [en_sentence for en_sentence, _ in text_pairs[:20000]]\n# all_en_sentences = ' '.join(en_sentences)\n# eng_doc = nlp(all_en_sentences)\n# unique_eng_tokens = set(token.text for token in eng_doc)\n# len(unique_eng_tokens)\n\n## ---> All english sentences (~160K) contain together ~14.5K unique tokens\n## ---> All french sentences (~160K) contain together >25K unique tokens","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:22.529340Z","iopub.execute_input":"2024-01-14T21:19:22.529569Z","iopub.status.idle":"2024-01-14T21:19:22.548072Z","shell.execute_reply.started":"2024-01-14T21:19:22.529549Z","shell.execute_reply":"2024-01-14T21:19:22.547257Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nNow, let's split the sentence pairs into a training set, a validation set,\nand a test set.\n\"\"\"\nrandom.seed(123)\nrandom.shuffle(text_pairs)\nnum_val_samples = int(0.15 * len(text_pairs))\nnum_train_samples = len(text_pairs) - 2 * num_val_samples\ntrain_pairs = text_pairs[:num_train_samples]\nval_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\ntest_pairs = text_pairs[num_train_samples + num_val_samples :]\n\nprint(f\"{len(text_pairs)} total text pairs\")\nprint(f\"{len(train_pairs)} training pairs\")\nprint(f\"{len(val_pairs)} validation pairs\")\nprint(f\"{len(test_pairs)} test pairs\")\nprint(\"THIS TRAIN TEST VAL SPLIT IS NOT USED YET (cf val=0.2 : automatic split in the keras model def)\")","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:22.549187Z","iopub.execute_input":"2024-01-14T21:19:22.549441Z","iopub.status.idle":"2024-01-14T21:19:22.702252Z","shell.execute_reply.started":"2024-01-14T21:19:22.549419Z","shell.execute_reply":"2024-01-14T21:19:22.701430Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"160538 total text pairs\n112378 training pairs\n24080 validation pairs\n24080 test pairs\nTHIS TRAIN TEST VAL SPLIT IS NOT USED YET (cf val=0.2 : automatic split in the keras model def)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **TextVecotrization: tokenization & index representation**","metadata":{}},{"cell_type":"code","source":"strip_chars = list(string.punctuation)\neng_strip_chars = '  '.join(strip_chars)\nstrip_chars.remove('[')\nstrip_chars.remove(']')\nfre_strip_chars = '  '.join(strip_chars)","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:22.706047Z","iopub.execute_input":"2024-01-14T21:19:22.706316Z","iopub.status.idle":"2024-01-14T21:19:22.712702Z","shell.execute_reply.started":"2024-01-14T21:19:22.706294Z","shell.execute_reply":"2024-01-14T21:19:22.712017Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(f\"Characters to be deleted from english sequences:  {eng_strip_chars}\")\nprint(f\"Characters to be deleted from french sequences:  {fre_strip_chars}\")\n\n#There's a default standardization done by keras.layers.TextVectorization (we'll apply it to english sentences)\n#But it deletes also '[' and ']' symbols, that's why we specify a custom standardization for french sentences (we want to keep [start] and [end] tokens)\ndef custom_standardization(input_string):\n    lowercase = tf_strings.lower(input_string)\n    return tf_strings.regex_replace(lowercase, \"[%s]\" % re.escape(''.join(strip_chars)), \"\")\n\n#testing the function\nprint(custom_standardization(\"[start] Vas y, cours 'plus' vite !! [end]\"))","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:22.713718Z","iopub.execute_input":"2024-01-14T21:19:22.714051Z","iopub.status.idle":"2024-01-14T21:19:22.731044Z","shell.execute_reply.started":"2024-01-14T21:19:22.714020Z","shell.execute_reply":"2024-01-14T21:19:22.730028Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Characters to be deleted from english sequences:  !  \"  #  $  %  &  '  (  )  *  +  ,  -  .  /  :  ;  <  =  >  ?  @  [  \\  ]  ^  _  `  {  |  }  ~\nCharacters to be deleted from french sequences:  !  \"  #  $  %  &  '  (  )  *  +  ,  -  .  /  :  ;  <  =  >  ?  @  \\  ^  _  `  {  |  }  ~\ntf.Tensor(b'[start] vas y cours plus vite  [end]', shape=(), dtype=string)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Vectorization Parameters","metadata":{}},{"cell_type":"code","source":"vocab_size = 20000\nsequence_length = 20 #20 suggested in litterature","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:22.732321Z","iopub.execute_input":"2024-01-14T21:19:22.732596Z","iopub.status.idle":"2024-01-14T21:19:22.736278Z","shell.execute_reply.started":"2024-01-14T21:19:22.732575Z","shell.execute_reply":"2024-01-14T21:19:22.735465Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"\"\"\"Vectorizing text data using Keras TextVectorization -> Representing unique tokens with indices in a dictionnary\"\"\"\n\n#Default standardization for eng (strip string.punctuations )\neng_vectorization = TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length,\n)\n#customized for frenish\nfre_vectorization = TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length + 1,\n    standardize=custom_standardization,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:22.737422Z","iopub.execute_input":"2024-01-14T21:19:22.737671Z","iopub.status.idle":"2024-01-14T21:19:22.768807Z","shell.execute_reply.started":"2024-01-14T21:19:22.737649Z","shell.execute_reply":"2024-01-14T21:19:22.767985Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Training the vectorizers","metadata":{}},{"cell_type":"code","source":"train_eng_texts = [pair[0] for pair in text_pairs]  #Change text_pairs to train_pairs if I want to use the train-test-val split\ntrain_fre_texts = [pair[1] for pair in text_pairs]\n\neng_vectorization.adapt(train_eng_texts) #fitting the text vectorization layer to data\nfre_vectorization.adapt(train_fre_texts) #same (but unchanged data)","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:22.769983Z","iopub.execute_input":"2024-01-14T21:19:22.770560Z","iopub.status.idle":"2024-01-14T21:19:48.127955Z","shell.execute_reply.started":"2024-01-14T21:19:22.770529Z","shell.execute_reply":"2024-01-14T21:19:48.127096Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Downloading the vectorizers","metadata":{}},{"cell_type":"code","source":"\"\"\"This cell downloads the two TextVectorizer that were trained on our text data\"\"\"\n##(déjà fait pour vocab_size=15000 et sentence_length=14)\n## Sauvegarder le vectorizer anglais ##\n\n# eng_vectorization_data = {\n#     'config': eng_vectorization.get_config(),\n#     'weights': eng_vectorization.get_weights()\n# }\n# joblib.dump(eng_vectorization_data, 'text_vectorizer_eng_20k-vocab20.joblib')\n\n# # Sauvegarder le vectorizer français ##\n# fre_vectorization_data = {\n#     'config': fre_vectorization.get_config(),\n#     'weights': fre_vectorization.get_weights()\n# }\n# joblib.dump(fre_vectorization_data, 'text_vectorizer_fr_20k-vocab20.joblib')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-14T22:21:20.553443Z","iopub.execute_input":"2024-01-14T22:21:20.554445Z","iopub.status.idle":"2024-01-14T22:21:20.706104Z","shell.execute_reply.started":"2024-01-14T22:21:20.554411Z","shell.execute_reply":"2024-01-14T22:21:20.705155Z"},"trusted":true},"execution_count":96,"outputs":[{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"['text_vectorizer_fr_20k-vocab20.joblib']"},"metadata":{}}]},{"cell_type":"code","source":"# Get the vocabulary of the text vectorization layer\nvocabulary = fre_vectorization.get_vocabulary()\nprint(\"french vocab: \", len(vocabulary))\nprint(\"english vocab: \", len(eng_vectorization.get_vocabulary()))\n\n# vocab_size = min(len(eng_vectorization.get_vocabulary()), len(vocabulary))\n\n# Check if \"[start]\" is in the vocabulary\nis_start_token_in_vocab = \"[start]\" in vocabulary\n\n# Print the result\nprint(f\"[start] is considered as a single token: {is_start_token_in_vocab}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:48.137663Z","iopub.execute_input":"2024-01-14T21:19:48.138009Z","iopub.status.idle":"2024-01-14T21:19:48.234401Z","shell.execute_reply.started":"2024-01-14T21:19:48.137964Z","shell.execute_reply":"2024-01-14T21:19:48.233502Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"french vocab:  20000\nenglish vocab:  14341\n[start] is considered as a single token: True\n","output_type":"stream"}]},{"cell_type":"code","source":"# \"\"\"Next, we'll format our datasets.\n\n# At each training step, the model will seek to predict target words N+1 (and beyond)\n# using the source sentence and the target words 0 to N.\n\n# As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n\n# - `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n# `encoder_inputs` is the vectorized source sentence and `encoder_inputs` is the target sentence \"so far\",\n# that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n# - `target` is the target sentence offset by one step:\n# it provides the next words in the target sentence -- what the model will try to predict.\n# \"\"\"\n# def format_dataset(eng, fre):\n#     eng = eng_vectorization(eng)\n#     fre = fre_vectorization(fre)\n#     return (\n#         {\n#             \"encoder_inputs\": eng,\n#             \"decoder_inputs\": fre[:, :-1],\n#         },\n#         fre[:, 1:],\n#     )","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:48.235544Z","iopub.execute_input":"2024-01-14T21:19:48.235813Z","iopub.status.idle":"2024-01-14T21:19:48.241096Z","shell.execute_reply.started":"2024-01-14T21:19:48.235791Z","shell.execute_reply":"2024-01-14T21:19:48.240134Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# **IV. Model Definition & Training**","metadata":{}},{"cell_type":"markdown","source":"## **IV.1 Applying Vectorizers, Padding sequences, Encoder-Decoder Definition**\n### Encoder input\nEncoder Input: Sequences of English tokens (sentences) represented as integer indices.","metadata":{}},{"cell_type":"code","source":"# Data preparation ( stripping, tokenization and vectorization(indexing) )\ntrain_eng_sequences = eng_vectorization(train_eng_texts)\n# Pad the sequences to the specified sequence length\nstart = time.time()\nencoder_input_data = pad_sequences(train_eng_sequences, maxlen=sequence_length, padding=\"post\")\nend = time.time()\nprint(f\"Incoder pad_sequences execution time : {end - start:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:19:48.242334Z","iopub.execute_input":"2024-01-14T21:19:48.242595Z","iopub.status.idle":"2024-01-14T21:22:50.097925Z","shell.execute_reply.started":"2024-01-14T21:19:48.242573Z","shell.execute_reply":"2024-01-14T21:22:50.096872Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Incoder pad_sequences execution time : 180.66\n","output_type":"stream"}]},{"cell_type":"code","source":"len(encoder_input_data[0])","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:22:50.098985Z","iopub.execute_input":"2024-01-14T21:22:50.099250Z","iopub.status.idle":"2024-01-14T21:22:50.105197Z","shell.execute_reply.started":"2024-01-14T21:22:50.099228Z","shell.execute_reply":"2024-01-14T21:22:50.104354Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"20"},"metadata":{}}]},{"cell_type":"markdown","source":"### Decoder input\nDecoder Input: Sequences of french sentences represented as integer indices.\nSHIFTED BY 1 POSITION !","metadata":{}},{"cell_type":"code","source":"# Data preparation\ntrain_fre_sequences = fre_vectorization(train_fre_texts)\n# Shift the target french sentences by one position (decoder input)\ndecoder_input_data = train_fre_sequences[:, :-1]\n# Pad the sequences to the specified sequence length (+1 for the start token)\n# decoder_input_data = pad_sequences(decoder_input_data, maxlen=sequence_length + 1, padding=\"post\")\nstart = time.time()\ndecoder_input_data = pad_sequences(decoder_input_data, maxlen=sequence_length, padding=\"post\")\nend = time.time()\nprint(f\"decoder pad_sequences execution time : {end - start:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:22:50.106293Z","iopub.execute_input":"2024-01-14T21:22:50.106541Z","iopub.status.idle":"2024-01-14T21:25:51.238689Z","shell.execute_reply.started":"2024-01-14T21:22:50.106520Z","shell.execute_reply":"2024-01-14T21:25:51.237730Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"decoder pad_sequences execution time : 179.87\n","output_type":"stream"}]},{"cell_type":"code","source":"len(decoder_input_data[0])","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:25:51.239990Z","iopub.execute_input":"2024-01-14T21:25:51.240352Z","iopub.status.idle":"2024-01-14T21:25:51.246959Z","shell.execute_reply.started":"2024-01-14T21:25:51.240320Z","shell.execute_reply":"2024-01-14T21:25:51.246017Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"20"},"metadata":{}}]},{"cell_type":"markdown","source":"### Decoder Output\n\nDecoder Output: Target French sentences (with hiding the first token). \\\nN.B : I won't one-hot encode these sentences and use Categorical Crossentropy loss. To make it less expensive, I'll let tokens represented with indicies and I'll use Sparse Categorical Crossentropy","metadata":{}},{"cell_type":"code","source":"\"\"\"One-hot encoded representations can be an alternative (very expensive)\"\"\"\n\n# # One-hot encode the target french sentences (decoder output)\n# decoder_output_data = to_categorical(train_fre_sequences[:, 1:], num_classes=vocab_size)\n\n# decoder_output_data = decoder_output_data[:, :sequence_length + 1, :]  # Adjust the sequence length\ndecoder_output_data = train_fre_sequences[:, 1:]\nlen(decoder_output_data[0])","metadata":{"execution":{"iopub.status.busy":"2024-01-14T21:25:51.248099Z","iopub.execute_input":"2024-01-14T21:25:51.248397Z","iopub.status.idle":"2024-01-14T21:25:51.264173Z","shell.execute_reply.started":"2024-01-14T21:25:51.248362Z","shell.execute_reply":"2024-01-14T21:25:51.263228Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"20"},"metadata":{}}]},{"cell_type":"markdown","source":"## **IV.2 Model Definition**","metadata":{}},{"cell_type":"markdown","source":"#### Model parameters","metadata":{}},{"cell_type":"code","source":"embed_dim = 256 #should be equal to embed_dim (useless for now)\nlatent_dim = 176\nbatch_size = 128  # Batch size for training. \nepochs = 46  # Number of epochs to train for.\nnum_encoder_tokens = vocab_size  #max vocab of the original dataset (df_raw) for english is around 14.5k\nnum_decoder_tokens = vocab_size","metadata":{"execution":{"iopub.status.busy":"2024-01-14T22:34:03.667684Z","iopub.execute_input":"2024-01-14T22:34:03.668135Z","iopub.status.idle":"2024-01-14T22:34:03.672790Z","shell.execute_reply.started":"2024-01-14T22:34:03.668104Z","shell.execute_reply":"2024-01-14T22:34:03.672043Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"##Variant (word2Vec for embedding then no training of the embedding layer). (Cf. NLP from 0 to 1 : text classification and M.translation: Medium)\n# embedding_layer = Embedding(vocab_size, 150, weights=[embedding_vectors], input_length=max_length, trainable=False)\n# model = Sequential()\n# model.add(embedding_layer)\n# model.add(Dropout(0.2))","metadata":{"execution":{"iopub.status.busy":"2024-01-14T22:34:05.807512Z","iopub.execute_input":"2024-01-14T22:34:05.808342Z","iopub.status.idle":"2024-01-14T22:34:05.812219Z","shell.execute_reply.started":"2024-01-14T22:34:05.808310Z","shell.execute_reply":"2024-01-14T22:34:05.811263Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"# Need to have inputs = integer sequences (representing sequences of words, encoded by their index in a dictionary)\n# # Compile & run training\n# model.compile(optimizer='rmsprop', loss='categorical_crossentropy')  #if decoder_output_data was one-hot encoded\n# instantiating the model in the strategy scope creates the model on the TPU\n# with strategy.scope():\n    \n# Define an input sequence and process it.\nencoder_inputs = Input(shape=(None,))\nx = Embedding(num_encoder_tokens, latent_dim)(encoder_inputs)\nx, state_h, state_c = LSTM(latent_dim,\n                           return_state=True)(x)\nx = Dropout(0.3)(x)  # Add dropout layer to the encoder\n\nencoder_states = [state_h, state_c]\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None,))\nx = Embedding(num_decoder_tokens, latent_dim)(decoder_inputs)\nx = LSTM(latent_dim, return_sequences=True)(x, initial_state=encoder_states)\nx = Dropout(0.3)(x)  # Add dropout layer to the decoder\ndecoder_outputs = Dense(num_decoder_tokens, activation='softmax')(x)\n\n# Define the model that will turn\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n# Compile the model with sparse categorical crossentropy\noptimizer = RMSprop(learning_rate=0.002)\nmodel.compile(optimizer=optimizer, loss=sparse_categorical_crossentropy)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-01-14T22:34:10.272738Z","iopub.execute_input":"2024-01-14T22:34:10.273404Z","iopub.status.idle":"2024-01-14T22:34:10.802048Z","shell.execute_reply.started":"2024-01-14T22:34:10.273374Z","shell.execute_reply":"2024-01-14T22:34:10.801242Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-14T22:34:11.956266Z","iopub.execute_input":"2024-01-14T22:34:11.956615Z","iopub.status.idle":"2024-01-14T22:34:11.987178Z","shell.execute_reply.started":"2024-01-14T22:34:11.956588Z","shell.execute_reply":"2024-01-14T22:34:11.986301Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_3 (InputLayer)        [(None, None)]               0         []                            \n                                                                                                  \n input_4 (InputLayer)        [(None, None)]               0         []                            \n                                                                                                  \n embedding_2 (Embedding)     (None, None, 176)            3520000   ['input_3[0][0]']             \n                                                                                                  \n embedding_3 (Embedding)     (None, None, 176)            3520000   ['input_4[0][0]']             \n                                                                                                  \n lstm_2 (LSTM)               [(None, 176),                248512    ['embedding_2[0][0]']         \n                              (None, 176),                                                        \n                              (None, 176)]                                                        \n                                                                                                  \n lstm_3 (LSTM)               (None, None, 176)            248512    ['embedding_3[0][0]',         \n                                                                     'lstm_2[0][1]',              \n                                                                     'lstm_2[0][2]']              \n                                                                                                  \n dropout_3 (Dropout)         (None, None, 176)            0         ['lstm_3[0][0]']              \n                                                                                                  \n dense_1 (Dense)             (None, None, 20000)          3540000   ['dropout_3[0][0]']           \n                                                                                                  \n==================================================================================================\nTotal params: 11077024 (42.26 MB)\nTrainable params: 11077024 (42.26 MB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## **IV.3 Launching Training**","metadata":{}},{"cell_type":"code","source":"model.fit([encoder_input_data, decoder_input_data], decoder_output_data,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_split=0.18,\n          verbose=1)\n# strategy.run(replica_fn, args=dist_batch)","metadata":{"execution":{"iopub.status.busy":"2024-01-14T22:34:23.915980Z","iopub.execute_input":"2024-01-14T22:34:23.916351Z","iopub.status.idle":"2024-01-14T23:10:12.636009Z","shell.execute_reply.started":"2024-01-14T22:34:23.916321Z","shell.execute_reply":"2024-01-14T23:10:12.635013Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"Epoch 1/46\n1029/1029 [==============================] - 66s 61ms/step - loss: 2.2171 - val_loss: 1.9392\nEpoch 2/46\n1029/1029 [==============================] - 47s 46ms/step - loss: 1.8122 - val_loss: 1.6863\nEpoch 3/46\n1029/1029 [==============================] - 47s 45ms/step - loss: 1.6291 - val_loss: 1.5452\nEpoch 4/46\n1029/1029 [==============================] - 47s 45ms/step - loss: 1.5127 - val_loss: 1.4599\nEpoch 5/46\n1029/1029 [==============================] - 47s 45ms/step - loss: 1.4149 - val_loss: 1.3462\nEpoch 6/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 1.3288 - val_loss: 1.2630\nEpoch 7/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 1.2522 - val_loss: 1.1994\nEpoch 8/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 1.1837 - val_loss: 1.1368\nEpoch 9/46\n1029/1029 [==============================] - 47s 45ms/step - loss: 1.1232 - val_loss: 1.0837\nEpoch 10/46\n1029/1029 [==============================] - 47s 45ms/step - loss: 1.0698 - val_loss: 1.0407\nEpoch 11/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 1.0210 - val_loss: 0.9996\nEpoch 12/46\n1029/1029 [==============================] - 47s 45ms/step - loss: 0.9787 - val_loss: 0.9624\nEpoch 13/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.9414 - val_loss: 0.9408\nEpoch 14/46\n1029/1029 [==============================] - 47s 45ms/step - loss: 0.9072 - val_loss: 0.9046\nEpoch 15/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.8760 - val_loss: 0.8808\nEpoch 16/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.8481 - val_loss: 0.8604\nEpoch 17/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.8225 - val_loss: 0.8421\nEpoch 18/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.7988 - val_loss: 0.8289\nEpoch 19/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.7769 - val_loss: 0.8097\nEpoch 20/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.7560 - val_loss: 0.8011\nEpoch 21/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.7369 - val_loss: 0.7904\nEpoch 22/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.7191 - val_loss: 0.7830\nEpoch 23/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.7023 - val_loss: 0.7666\nEpoch 24/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.6862 - val_loss: 0.7586\nEpoch 25/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.6708 - val_loss: 0.7480\nEpoch 26/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.6566 - val_loss: 0.7432\nEpoch 27/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.6425 - val_loss: 0.7358\nEpoch 28/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.6296 - val_loss: 0.7285\nEpoch 29/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.6179 - val_loss: 0.7218\nEpoch 30/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.6056 - val_loss: 0.7145\nEpoch 31/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.5941 - val_loss: 0.7110\nEpoch 32/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.5829 - val_loss: 0.7092\nEpoch 33/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.5727 - val_loss: 0.7013\nEpoch 34/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.5625 - val_loss: 0.6995\nEpoch 35/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.5529 - val_loss: 0.6929\nEpoch 36/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.5435 - val_loss: 0.6893\nEpoch 37/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.5341 - val_loss: 0.6882\nEpoch 38/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.5254 - val_loss: 0.6845\nEpoch 39/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.5167 - val_loss: 0.6851\nEpoch 40/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.5094 - val_loss: 0.6826\nEpoch 41/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.5015 - val_loss: 0.6782\nEpoch 42/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.4932 - val_loss: 0.6764\nEpoch 43/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.4864 - val_loss: 0.6756\nEpoch 44/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.4796 - val_loss: 0.6718\nEpoch 45/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.4723 - val_loss: 0.6705\nEpoch 46/46\n1029/1029 [==============================] - 46s 45ms/step - loss: 0.4657 - val_loss: 0.6706\n","output_type":"stream"},{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x7fc07397f850>"},"metadata":{}}]},{"cell_type":"markdown","source":"# **V. Inference:**\n##### Encoder Input: A single English sentence represented as integer indices.\n##### Decoder Input: A start-of-sequence token (initially) and subsequently predicted tokens (Recurrency).\n##### Decoder Output: Predicted probabilities for the next word in the sequence.","metadata":{}},{"cell_type":"code","source":"def translate_sentence(model, eng_text, eng_vectorization, fre_vectorization, sequence_length):\n    \n    # Tokenize and pad the English input sentence\n    eng_sequence = eng_vectorization(np.array([eng_text]))\n    eng_sequence = pad_sequences(eng_sequence, maxlen=sequence_length, padding=\"post\")\n\n    # Initialize the decoder input with the start token\n    fre_sequence = np.zeros((1, sequence_length), dtype=np.int32)\n    fre_sequence[0, 0] = fre_vectorization.get_vocabulary().index('[start]')\n\n    # Inference loop\n    for i in range(1, sequence_length):\n        predictions = model.predict([eng_sequence, fre_sequence])\n        predicted_token_index = np.argmax(predictions[0, i - 1, :])\n        fre_sequence[0, i] = predicted_token_index\n\n        # Check for the end token\n        if fre_vectorization.get_vocabulary()[predicted_token_index] == '[end]':\n            break\n\n    # Convert the predicted indices to French text\n    translated_text = ' '.join([fre_vectorization.get_vocabulary()[idx] for idx in fre_sequence[0] if idx > 0])\n    if not(\"[end]\" in translated_text):\n        translated_text+=\" [end]\"\n    \n    #further processing (without the trained model) of the sentence....\n    # to think about (ex. change cest with c'est in the french sentence)  (renverse preprocessing)\n    \n\n    return translated_text\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-14T22:09:57.646641Z","iopub.execute_input":"2024-01-14T22:09:57.647532Z","iopub.status.idle":"2024-01-14T22:09:57.655920Z","shell.execute_reply.started":"2024-01-14T22:09:57.647501Z","shell.execute_reply":"2024-01-14T22:09:57.654994Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"#### **Inference tests of the trained Model**","metadata":{}},{"cell_type":"code","source":"#test before saving\n# Example usage\neng_text_to_translate = \"It's very beautiful\" \ntranslated_sentence = translate_sentence(model, eng_text_to_translate, eng_vectorization, fre_vectorization, sequence_length)\nprint(\"Translated Sentence (current model): \", translated_sentence)","metadata":{"execution":{"iopub.status.busy":"2024-01-14T22:19:03.226201Z","iopub.execute_input":"2024-01-14T22:19:03.226595Z","iopub.status.idle":"2024-01-14T22:19:03.986831Z","shell.execute_reply.started":"2024-01-14T22:19:03.226568Z","shell.execute_reply":"2024-01-14T22:19:03.985879Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 20ms/step\n1/1 [==============================] - 0s 20ms/step\nTranslated Sentence (current model):  [start] cest très beau [end]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Saving the trained model :\n##### Be careful to save only after complete training and getting good performances","metadata":{}},{"cell_type":"code","source":"# Save the model\nmodel.save('my_translation_model_gpu_v80.h5')","metadata":{"execution":{"iopub.status.busy":"2024-01-14T22:19:22.921754Z","iopub.execute_input":"2024-01-14T22:19:22.922606Z","iopub.status.idle":"2024-01-14T22:19:23.058796Z","shell.execute_reply.started":"2024-01-14T22:19:22.922570Z","shell.execute_reply":"2024-01-14T22:19:23.058023Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"### Loading a model for inference","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport os\nprint(os.listdir('/kaggle/input'))","metadata":{"execution":{"iopub.status.busy":"2024-01-14T15:18:07.076699Z","iopub.execute_input":"2024-01-14T15:18:07.077102Z","iopub.status.idle":"2024-01-14T15:18:07.082732Z","shell.execute_reply.started":"2024-01-14T15:18:07.077058Z","shell.execute_reply":"2024-01-14T15:18:07.081805Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"['tabdelimited-englisharabic-sentence-pairs', 'translation-of-sentences-in-different-languages', 'my-model-v6-0-97-val-loss-and-0-3-train-loss']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the model for inference\n# model_v01 = load_model('my_translation_model_v01.h5')\n\n# model_gpu_v02 = load_model('my_translation_model_gpu_v02.h5')   #big (20k vocab and 50 epochs, 32min of training with cpu+1gpu)\n# model_gpu_v3 = load_model('my_translation_model_gpu_v3')   #big (14k vocab and 35 epochs, 18min of training with cpu+1gpu, lr=0.005, 0.2 train_loss/ 1.01 val_loss/ 512 latent dim) : not bad nor good\n\n# model_gpu_v6 = load_model('my_translation_model_gpu_v6.h5')   #big (14k vocab and 30 epochs, .... of training with cpu+1gpu, lr=0.001, 0.41 train_loss/ 0.97 val_loss/ 256 latent dim) Impression : not bad\n\n#model_gpu_v7     #Big : 20k vocab, 27 epochs, gpu, lr=0.02, 0.6 train_loss, 0.93 val_loss, 256 embedding/hidden, dropout(0.2)\n\nmodel_gpu_v6 = load_model('/kaggle/input/my-model-v6-0-97-val-loss-and-0-3-train-loss/my_translation_model_gpu_v6.h5') \n\n\n# model_v03 = load_model('my_translation_model_v03')  #medium (10k vocab and 40 epochs, less than 10min of training with CPUs)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T20:28:01.031205Z","iopub.execute_input":"2024-01-11T20:28:01.031959Z","iopub.status.idle":"2024-01-11T20:28:03.291128Z","shell.execute_reply.started":"2024-01-11T20:28:01.031926Z","shell.execute_reply":"2024-01-11T20:28:03.290289Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# # Example usage\n# eng_text_to_translate = \"don't talk to me\"\n# translated_sentence = translate_sentence(model_gpu_v02, eng_text_to_translate, eng_vectorization, fre_vectorization, sequence_length)\n# print(\"Translated Sentence (gpu_v02): \", translated_sentence)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T22:43:14.038322Z","iopub.execute_input":"2024-01-03T22:43:14.039029Z","iopub.status.idle":"2024-01-03T22:43:14.042827Z","shell.execute_reply.started":"2024-01-03T22:43:14.038996Z","shell.execute_reply":"2024-01-03T22:43:14.041971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Example usage\n# eng_text_to_translate = \"say good words\"\n# translated_sentence = translate_sentence(model_gpu_v3, eng_text_to_translate, eng_vectorization, fre_vectorization, sequence_length)\n# print(\"Translated Sentence (gpu_v3): \", translated_sentence)\n# # for _ in range(30):\n# #     input_sentence = random.choice(test_eng_texts)\n# #     print( \"Translated Sentence (gpu_v3): \", decode_sequence(input_sentence)() )","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-01-03T22:43:14.259028Z","iopub.execute_input":"2024-01-03T22:43:14.259327Z","iopub.status.idle":"2024-01-03T22:43:14.263684Z","shell.execute_reply.started":"2024-01-03T22:43:14.259302Z","shell.execute_reply":"2024-01-03T22:43:14.262897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage\neng_text_to_translate = \"I dream about you every night\"\ntranslated_sentence = translate_sentence(model_gpu_v6, eng_text_to_translate, eng_vectorization, fre_vectorization, sequence_length)\nprint(\"Translated Sentence (gpu_v6): \", translated_sentence)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T22:46:24.618210Z","iopub.execute_input":"2024-01-03T22:46:24.618862Z","iopub.status.idle":"2024-01-03T22:46:25.790938Z","shell.execute_reply.started":"2024-01-03T22:46:24.618829Z","shell.execute_reply":"2024-01-03T22:46:25.789620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Checking if a specific word is in vocabulary\"\"\"\nvocab_fra = fre_vectorization.get_vocabulary()\nvocab_eng = eng_vectorization.get_vocabulary()\nprint(len(vocab_eng))\n'babe' in vocab_eng","metadata":{"execution":{"iopub.status.busy":"2024-01-14T19:53:56.125797Z","iopub.execute_input":"2024-01-14T19:53:56.126205Z","iopub.status.idle":"2024-01-14T19:53:56.164614Z","shell.execute_reply.started":"2024-01-14T19:53:56.126168Z","shell.execute_reply":"2024-01-14T19:53:56.163373Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Checking if a specific word is in vocabulary\"\"\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m vocab_fra \u001b[38;5;241m=\u001b[39m \u001b[43mfre_vectorization\u001b[49m\u001b[38;5;241m.\u001b[39mget_vocabulary()\n\u001b[1;32m      3\u001b[0m vocab_eng \u001b[38;5;241m=\u001b[39m eng_vectorization\u001b[38;5;241m.\u001b[39mget_vocabulary()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(vocab_eng))\n","\u001b[0;31mNameError\u001b[0m: name 'fre_vectorization' is not defined"],"ename":"NameError","evalue":"name 'fre_vectorization' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"\"\" Proposition for adapting this model by integrating transformers\"\"\"\n# import tensorflow as tf\n# from tensorflow import keras\n# from tensorflow.keras import layers\n\n# # ... (previous code remains unchanged)\n\n# # Encoder\n# encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n# encoder_embedding = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)(encoder_inputs)\n# encoder_lstm = layers.LSTM(latent_dim, return_state=True)\n# encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n# encoder_states = [state_h, state_c]\n\n# encoder = keras.Model(encoder_inputs, encoder_states)\n\n# # Decoder\n# decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n# decoder_embedding = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)(decoder_inputs)\n# decoder_lstm = layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n# decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n# decoder_dense = layers.Dense(vocab_size, activation=\"softmax\")\n# decoder_outputs = decoder_dense(decoder_outputs)\n\n# decoder = keras.Model(decoder_inputs, decoder_outputs)\n\n# # Model\n# decoder_outputs = decoder(decoder_inputs)\n# lstm_transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"lstm_transformer\")\n\n# # ... (rest of the code remains unchanged)\n\n# # Training the model\n# lstm_transformer.summary()\n# lstm_transformer.compile(\n#     \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n# )\n# lstm_transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)\n\n# # ... (rest of the code remains unchanged)\n\n# # Decoding test sentences\n# fre_vocab = fre_vectorization.get_vocabulary()\n# fre_index_lookup = dict(zip(range(len(fre_vocab)), fre_vocab))\n# max_decoded_sentence_length = 20\n\n# # ... (rest of the code remains unchanged)\n","metadata":{},"execution_count":null,"outputs":[]}]}